{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad Based Visualization of CNN-like neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define surf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "os.environ[\"TORCH_HOME\"] = \".\"\n",
    "import sys\n",
    "\n",
    "# torch.cuda.is_available()\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "############################################\n",
    "# parameters to be set for encironment\n",
    "# NOT NEED to modify if you have the setting\n",
    "\n",
    "data_root_path = \"/opt/local/torch/data/tcr-mat-form\"  # path to store train data\n",
    "util_path = \"../../..\"  # path to store the util package\n",
    "############################################\n",
    "\n",
    "# add local dir to sys path\n",
    "sys.path.insert(0, util_path)  # the util package is supposed to be clone to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.model.surf.modified_cnn_model import (\n",
    "    ModifiedPretrainedNet,\n",
    "    SurfNet256,\n",
    ")\n",
    "from typing import Any, Dict, Union\n",
    "from util.model.surf.dateset import SurfDatasetFromMat\n",
    "from util.model.surf.pretrained_model import PretrainedModelDb\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def define_surf_model(\n",
    "    model_name: str,\n",
    "    model_type: Union[int, str],\n",
    "    suffix: str,\n",
    "    pretrain: bool = True,\n",
    "    set_type: str = \"train\",\n",
    "    kwd: Dict[str, Any] = {\n",
    "        \"dropout\": 0.2,\n",
    "        \"cnn_feature_ratio\": 0.5,\n",
    "        \"data_root_path\": \"/hy-tmp/\",\n",
    "        \"data_csv_filename\": \"DataNormilized.csv\",\n",
    "        \"lr\": 0.001,\n",
    "        \"num_params\": 3,\n",
    "        \"num_output\": 2,\n",
    "    },\n",
    "):\n",
    "    \"\"\"\n",
    "    定义和配置一个用于表面模型的深度学习模型。\n",
    "\n",
    "    参数:\n",
    "    - model_name: 模型的名称。\n",
    "    - model_type: 模型的类型，可以是整数或字符串。\n",
    "    - suffix: 附加在模型名称和类型之后的后缀，用于区分不同的模型配置。\n",
    "    - pretrain: 是否使用预训练的模型权重，默认为True。\n",
    "    - kwd: 包含模型训练和配置所需的各种参数的字典。\n",
    "\n",
    "    返回:\n",
    "    一个包含模型配置和训练所需信息的字典，包括模型前缀、模型实例、数据集、优化器和损失函数。\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化预训练模型数据库\n",
    "    model_info_db = PretrainedModelDb()\n",
    "\n",
    "    # 从数据库中获取指定模型和类型的信息\n",
    "    train_model, model_weights, name_first_conv, name_fc = model_info_db.get_info(\n",
    "        model_name, model_type\n",
    "    )\n",
    "\n",
    "    # 构造模型前缀，用于后续的日志记录或模型保存\n",
    "    prefix = f\"{model_name}{model_type}_{suffix}\"\n",
    "\n",
    "    # 创建一个修改过的预训练网络实例\n",
    "    pnet = ModifiedPretrainedNet(\n",
    "        pretrained_net=train_model,\n",
    "        weights=model_weights if pretrain else None,\n",
    "        name_first_conv=name_first_conv,\n",
    "        name_fc=name_fc,\n",
    "    )\n",
    "\n",
    "    # 创建并配置最终的表面模型\n",
    "    surf_model = SurfNet256(\n",
    "        modified_net=pnet,\n",
    "        num_params=kwd[\"num_params\"],\n",
    "        num_output=kwd[\"num_output\"],\n",
    "        dropout=kwd[\"dropout\"],\n",
    "        cnn_feature_ratio=kwd[\"cnn_feature_ratio\"],\n",
    "    )\n",
    "\n",
    "    # 将模型移动到指定的设备上\n",
    "    surf_model.to(device)\n",
    "\n",
    "    # 创建并配置数据集\n",
    "    dset = SurfDatasetFromMat(\n",
    "        data_csv_filename=os.path.join(\n",
    "            kwd[\"data_root_path\"], set_type, kwd[\"data_csv_filename\"]\n",
    "        ),\n",
    "        surf_data_dir=os.path.join(kwd[\"data_root_path\"], set_type, \"Surf\"),\n",
    "        param_start_idx=3,\n",
    "        param_end_idx=6,\n",
    "        num_targets=2,\n",
    "    )\n",
    "\n",
    "    # 创建优化器，用于模型参数的更新\n",
    "    optimizer = optim.Adam(surf_model.parameters(), lr=kwd[\"lr\"])\n",
    "\n",
    "    # 定义损失函数，用于衡量模型预测值与真实值的差异\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    # 返回包含模型配置和训练所需信息的字典\n",
    "    return {\n",
    "        \"prefix\": prefix,\n",
    "        \"train_model\": surf_model,\n",
    "        \"dset\": dset,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"loss_func\": loss_func,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trained checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwd = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"cnn_feature_ratio\": 0.5,\n",
    "    \"data_root_path\": data_root_path,\n",
    "    \"data_csv_filename\": \"DataNormilized.csv\",\n",
    "    \"lr\": 0.001,\n",
    "    \"num_params\": 3,\n",
    "    \"num_output\": 2,\n",
    "}\n",
    "suffix = \"input254_cv5_train10000\"\n",
    "model_info = define_surf_model(\n",
    "    model_name=\"densenet\",\n",
    "    model_type=\"121\",\n",
    "    pretrain=False,\n",
    "    suffix=suffix,\n",
    "    set_type=\"test\",\n",
    "    kwd=kwd,\n",
    ")\n",
    "model_name = f\"{model_info['prefix']}\"\n",
    "root_path = \"../../../thesis/surfTopo/checkpoints\"\n",
    "\n",
    "best_checkpoint = torch.load(\n",
    "    os.path.join(root_path, model_name, f\"surf_{model_name}_best.ckpt\")\n",
    ")\n",
    "# latest_checkpoint = torch.load(\n",
    "#     os.path.join(root_path, model_name, f\"surf_{model_name}_latest.ckpt\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_model = model_info[\"train_model\"]\n",
    "surf_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
    "optimizer = model_info[\"optimizer\"]\n",
    "optimizer.load_state_dict(best_checkpoint[\"optimizer_state_dict\"])\n",
    "test_set = model_info[\"dset\"]\n",
    "loss_func = model_info[\"loss_func\"]\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model preprocessing\n",
    "\n",
    "* 根据反向传播的规则,需要ReLU层的inplace=False,因此做如下修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in surf_model.named_modules():\n",
    "    if isinstance(module, nn.ReLU):\n",
    "        # print(name)\n",
    "        setattr(module, \"inplace\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Backpropagation 导向反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.visual.torch.cnn.backprop import GuidedBackprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define visualization class 定义可视化类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbp = GuidedBackprop(\n",
    "    model=surf_model, relu_modules=[surf_model.pretrained_net], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import one case in test set 从参数集中导入一个测试用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "ReLU forward hook called\n",
      "tensor(8.3125e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "ReLU back hook called\n",
      "First conv layer hooked\n",
      "release all hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    grad, grad_times_image = gbp.generate_gradients(batch, loss_func=loss_func)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grad of guided backpropagation 导向反向传播的梯度可视化\n",
    "\n",
    "包括整体梯度，正向和负向影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from util.visual.torch.cnn.functions import get_positive_negative_saliency\n",
    "\n",
    "# 创建一个图形\n",
    "\n",
    "input_surf = batch[0][0].detach().numpy()\n",
    "pos, neg = get_positive_negative_saliency(grad)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "input_kwds = {\"cmap\": \"gist_rainbow\"}\n",
    "grad_kwds = {\"alpha\": 0.5, \"cmap\": \"gist_rainbow\"}\n",
    "\n",
    "# 显示第一张图片\n",
    "axes[0, 0].imshow(input_surf[0], zorder=0, **input_kwds)\n",
    "axes[0, 1].imshow(grad[0], zorder=1, **grad_kwds)\n",
    "axes[0, 2].imshow(pos[0], zorder=1, **grad_kwds)\n",
    "axes[0, 3].imshow(neg[0], zorder=1, **grad_kwds)\n",
    "# 显示第二张图片\n",
    "axes[1, 0].imshow(input_surf[1], zorder=0, **input_kwds)\n",
    "axes[1, 1].imshow(grad[1], zorder=1, **grad_kwds)\n",
    "axes[1, 2].imshow(pos[1], zorder=1, **grad_kwds)\n",
    "axes[1, 3].imshow(neg[1], zorder=1, **grad_kwds)\n",
    "#\n",
    "axes[2, 0].imshow(input_surf[0] + input_surf[1], zorder=0, **input_kwds)\n",
    "axes[2, 1].imshow(grad[0] + grad[1], zorder=1, **grad_kwds)\n",
    "axes[2, 2].imshow(pos[0] + pos[1], zorder=1, **grad_kwds)\n",
    "axes[2, 3].imshow(neg[0] + neg[1], zorder=1, **grad_kwds)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "# 显示图形\n",
    "\n",
    "plt.savefig(\"./fig/gbp.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grad times image of guided backpropagation 梯度x原表面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "# 显示第一张图片\n",
    "axes[0, 0].imshow(input_surf[0], zorder=0, **input_kwds)\n",
    "axes[0, 1].imshow(grad_times_image[0], zorder=1, **grad_kwds)\n",
    "axes[0, 2].imshow(pos[0], zorder=1, **grad_kwds)\n",
    "axes[0, 3].imshow(neg[0], zorder=1, **grad_kwds)\n",
    "# 显示第二张图片\n",
    "axes[1, 0].imshow(input_surf[1], zorder=0, **input_kwds)\n",
    "axes[1, 1].imshow(grad_times_image[1], zorder=1, **grad_kwds)\n",
    "axes[1, 2].imshow(pos[1], zorder=1, **grad_kwds)\n",
    "axes[1, 3].imshow(neg[1], zorder=1, **grad_kwds)\n",
    "#\n",
    "axes[2, 0].imshow(input_surf[0] + input_surf[1], zorder=0, **input_kwds)\n",
    "axes[2, 1].imshow(grad_times_image[0] + grad_times_image[1], zorder=1, **grad_kwds)\n",
    "axes[2, 2].imshow(pos[0] + pos[1], zorder=1, **grad_kwds)\n",
    "axes[2, 3].imshow(neg[0] + neg[1], zorder=1, **grad_kwds)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "# 显示图形\n",
    "\n",
    "plt.savefig(\"./fig/gbp_times_image.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smooth grad of guided backpropagation 平滑梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### single sigma 单个sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = 1\n",
    "smooth_grad = gbp.generate_smooth_grad(\n",
    "    batch, loss_func, num_samplers=50, sigma_multiplier=sm\n",
    ")\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# 显示第一张图片\n",
    "axes[0, 0].imshow(input_surf[0], zorder=0, **input_kwds)\n",
    "axes[0, 1].imshow(smooth_grad[0], zorder=1, **grad_kwds)\n",
    "\n",
    "# 显示第二张图片\n",
    "axes[1, 0].imshow(input_surf[1], zorder=0, **input_kwds)\n",
    "axes[1, 1].imshow(smooth_grad[1], zorder=1, **grad_kwds)\n",
    "\n",
    "#\n",
    "axes[2, 0].imshow(input_surf[0] + input_surf[1], zorder=0, **input_kwds)\n",
    "axes[2, 1].imshow(smooth_grad[0] + smooth_grad[1], zorder=1, **grad_kwds)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "# 显示图形\n",
    "\n",
    "plt.savefig(f\"./fig/gbp_smooth_grad_sigma_{sm}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### make gif for one of the surf 制作其中一个表面的gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm in range(1, 100):\n",
    "    smooth_grad = gbp.generate_smooth_grad(\n",
    "        batch, loss_func, num_samplers=50, sigma_multiplier=sm\n",
    "    )\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    # 显示第一张图片\n",
    "    axes.imshow(smooth_grad[0], zorder=1, **grad_kwds)\n",
    "\n",
    "    axes.axis(\"off\")\n",
    "    # 显示图形\n",
    "    plt.title(f\"sigma={sm}/(max-min)\")\n",
    "    plt.savefig(f\"./fig/gbp_smooth_grad/sigma_{sm}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF 动画已保存到 ./fig/gbp_smooth_grad.gif\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 图片文件夹路径\n",
    "image_folder = \"./fig/gbp_smooth_grad\"\n",
    "\n",
    "# 获取文件夹中的所有图片文件\n",
    "images = [f\"sigma_{i}.png\" for i in range(1, 100)]\n",
    "\n",
    "# 读取图片并存储到列表中\n",
    "frames = []\n",
    "for image in images:\n",
    "    frame = Image.open(os.path.join(image_folder, image))\n",
    "    frames.append(frame)\n",
    "\n",
    "# 保存为 GIF 动画\n",
    "output_gif_path = \"./fig/gbp_smooth_grad.gif\"\n",
    "frames[0].save(\n",
    "    output_gif_path,\n",
    "    format=\"GIF\",\n",
    "    append_images=frames[1:],\n",
    "    save_all=True,\n",
    "    duration=1000,  # 每帧之间的间隔时间（毫秒）\n",
    "    loop=0,  # 循环次数，0 表示无限循环\n",
    ")\n",
    "\n",
    "print(f\"GIF 动画已保存到 {output_gif_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vanilla backpropagation 普通反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.visual.torch.cnn.backprop import VanillaBackprop\n",
    "\n",
    "vbp = VanillaBackprop(surf_model)\n",
    "for batch in test_loader:\n",
    "    g_v, grad_times_image_v = vbp.generate_gradients(batch, loss_func=loss_func)\n",
    "    break\n",
    "pos, neg = get_positive_negative_saliency(g_v)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "input_kwds = {\"cmap\": \"gist_rainbow\"}\n",
    "grad_kwds = {\"alpha\": 0.5, \"cmap\": \"gist_rainbow\"}\n",
    "\n",
    "# 显示第一张图片\n",
    "axes[0, 0].imshow(input_surf[0], zorder=0, **input_kwds)\n",
    "axes[0, 1].imshow(g_v[0], zorder=1, **grad_kwds)\n",
    "axes[0, 2].imshow(pos[0], zorder=1, **grad_kwds)\n",
    "axes[0, 3].imshow(neg[0], zorder=1, **grad_kwds)\n",
    "# 显示第二张图片\n",
    "axes[1, 0].imshow(input_surf[1], zorder=0, **input_kwds)\n",
    "axes[1, 1].imshow(g_v[1], zorder=1, **grad_kwds)\n",
    "axes[1, 2].imshow(pos[1], zorder=1, **grad_kwds)\n",
    "axes[1, 3].imshow(neg[1], zorder=1, **grad_kwds)\n",
    "#\n",
    "axes[2, 0].imshow(input_surf[0] + input_surf[1], zorder=0, **input_kwds)\n",
    "axes[2, 1].imshow(g_v[0] + g_v[1], zorder=1, **grad_kwds)\n",
    "axes[2, 2].imshow(pos[0] + pos[1], zorder=1, **grad_kwds)\n",
    "axes[2, 3].imshow(neg[0] + neg[1], zorder=1, **grad_kwds)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "# 显示图形\n",
    "plt.savefig(\"./fig/vbp.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### integrated gradients 集成梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    g_ig = vbp.generate_integrated_gradients(batch, loss_func=loss_func, steps=10)\n",
    "    break\n",
    "pos, neg = get_positive_negative_saliency(g_ig)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "input_kwds = {\"cmap\": \"gist_rainbow\"}\n",
    "grad_kwds = {\"alpha\": 0.5, \"cmap\": \"gist_rainbow\"}\n",
    "\n",
    "# 显示第一张图片\n",
    "axes[0, 0].imshow(input_surf[0], zorder=0, **input_kwds)\n",
    "axes[0, 1].imshow(g_ig[0], zorder=1, **grad_kwds)\n",
    "axes[0, 2].imshow(pos[0], zorder=1, **grad_kwds)\n",
    "axes[0, 3].imshow(neg[0], zorder=1, **grad_kwds)\n",
    "# 显示第二张图片\n",
    "axes[1, 0].imshow(input_surf[1], zorder=0, **input_kwds)\n",
    "axes[1, 1].imshow(g_ig[1], zorder=1, **grad_kwds)\n",
    "axes[1, 2].imshow(pos[1], zorder=1, **grad_kwds)\n",
    "axes[1, 3].imshow(neg[1], zorder=1, **grad_kwds)\n",
    "#\n",
    "axes[2, 0].imshow(input_surf[0] + input_surf[1], zorder=0, **input_kwds)\n",
    "axes[2, 1].imshow(g_ig[0] + g_ig[1], zorder=1, **grad_kwds)\n",
    "axes[2, 2].imshow(pos[0] + pos[1], zorder=1, **grad_kwds)\n",
    "axes[2, 3].imshow(neg[0] + neg[1], zorder=1, **grad_kwds)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "# 显示图形\n",
    "plt.savefig(\"./fig/ig.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
